{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_WElqiVR6xH",
        "outputId": "bed9a4c6-2d34-49d2-92c4-6e5438154c56"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAk7c6j3SPzP"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_9OKe7tSAT0"
      },
      "source": [
        "file_path = 'drive/MyDrive/data/anna.txt'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS_5D42eSAYu"
      },
      "source": [
        "import re\n",
        "\n",
        "sentences = []\n",
        "\n",
        "addressings_regexp = \" (Mr|St|Mrs|Ms|Dr)\\.$\"\n",
        "quotes = \"\\\"'\"\n",
        "end_of_sentence = \".!?\"\n",
        "\n",
        "text = text.lower()\n",
        "sentence = \"\"\n",
        "for symbol in text:\n",
        "  if symbol in quotes:\n",
        "    continue\n",
        "\n",
        "  if symbol == ':':\n",
        "    symbol = '.'\n",
        "\n",
        "  if symbol.isspace():\n",
        "    if len(sentence) != 0 and sentence[-1] == ' ':\n",
        "      continue\n",
        "    symbol = ' '\n",
        "\n",
        "  if not (symbol.isalpha() or symbol.isdigit() or symbol.isspace() or symbol in end_of_sentence):\n",
        "    if len(sentence) > 0 and sentence[-1] == ' ':\n",
        "      sentence = sentence[:-1]\n",
        "    if len(sentence) != 0 and sentence[-1] == ',':\n",
        "      continue\n",
        "    symbol = ','\n",
        "\n",
        "  sentence += symbol\n",
        "\n",
        "  if symbol in end_of_sentence:\n",
        "    suffix = \"\".join(sentence[-5:])\n",
        "    if re.search(addressings_regexp, suffix):\n",
        "      continue\n",
        "\n",
        "    letters_are_present = False\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "      if sentence[i].isalpha():\n",
        "        letters_are_present = True\n",
        "        break\n",
        "      i += 1\n",
        "\n",
        "    if not letters_are_present:\n",
        "      continue\n",
        "\n",
        "    sentences.append(sentence[i:])\n",
        "    sentence = \"\"\n",
        "\n",
        "limit = 70\n",
        "max_sentence_length = 0\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  if len(sentences[i]) > limit:\n",
        "    ind = limit\n",
        "    while ind < len(sentences[i]) and not sentences[i][ind].isspace():\n",
        "      ind += 1\n",
        "    sentences[i] = sentences[i][:ind]\n",
        "    if not sentences[i][-1] in end_of_sentence:\n",
        "      sentences[i] += '.'\n",
        "  max_sentence_length = max(max_sentence_length, len(sentences[i]))\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences[i] += '#' * (max_sentence_length - len(sentences[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqqyVrMeSAbF",
        "outputId": "e05c4ac9-81c3-42ce-c6ef-ec312690f06a"
      },
      "source": [
        "chars = list(set(\" \".join(sentences)))\n",
        "print(chars)\n",
        "int2char = dict(enumerate(chars))\n",
        "print(int2char)\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "print(char2int)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "encoded_sentences = np.array([[char2int[ch] for ch in sentence] for sentence in sentences])\n",
        "\n",
        "src = [sentence[:-1] for sentence in encoded_sentences]\n",
        "target = [sentence[1:] for sentence in encoded_sentences]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src, target))\n",
        "dataset = dataset.shuffle(100).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['n', '0', 'j', '4', 'p', 'y', 'k', 'l', 'q', '#', 'm', 'f', 'r', 'c', 'a', '1', 'e', '!', 'z', 'x', 'b', ',', 'g', '6', '8', 'h', 'i', 'w', '2', 'o', 'u', 't', ' ', 's', '3', 'v', '9', '7', '5', '?', '.', 'd']\n",
            "{0: 'n', 1: '0', 2: 'j', 3: '4', 4: 'p', 5: 'y', 6: 'k', 7: 'l', 8: 'q', 9: '#', 10: 'm', 11: 'f', 12: 'r', 13: 'c', 14: 'a', 15: '1', 16: 'e', 17: '!', 18: 'z', 19: 'x', 20: 'b', 21: ',', 22: 'g', 23: '6', 24: '8', 25: 'h', 26: 'i', 27: 'w', 28: '2', 29: 'o', 30: 'u', 31: 't', 32: ' ', 33: 's', 34: '3', 35: 'v', 36: '9', 37: '7', 38: '5', 39: '?', 40: '.', 41: 'd'}\n",
            "{'n': 0, '0': 1, 'j': 2, '4': 3, 'p': 4, 'y': 5, 'k': 6, 'l': 7, 'q': 8, '#': 9, 'm': 10, 'f': 11, 'r': 12, 'c': 13, 'a': 14, '1': 15, 'e': 16, '!': 17, 'z': 18, 'x': 19, 'b': 20, ',': 21, 'g': 22, '6': 23, '8': 24, 'h': 25, 'i': 26, 'w': 27, '2': 28, 'o': 29, 'u': 30, 't': 31, ' ': 32, 's': 33, '3': 34, 'v': 35, '9': 36, '7': 37, '5': 38, '?': 39, '.': 40, 'd': 41}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTDN2cRjXtSr"
      },
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFDQWt7-XFAW"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v17SmWVkXcNp"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = tf.one_hot(x, vocab_size)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN3ZYYlCC-X5"
      },
      "source": [
        "max_sentence_length -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW6DTrQbX5Ri"
      },
      "source": [
        "vocab_size = len(chars)\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(max_sentence_length,), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(max_sentence_length, vocab_size)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block1 = TransformerBlock(vocab_size, num_heads, feed_forward_dim)\n",
        "    x = transformer_block1(x)\n",
        "    transformer_block2 = TransformerBlock(vocab_size, num_heads, feed_forward_dim)\n",
        "    x = transformer_block2(x)\n",
        "    transformer_block3 = TransformerBlock(vocab_size, num_heads, feed_forward_dim)\n",
        "    x = transformer_block3(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtVLeM6xz7u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d459b8d-3b1d-47d3-fb25-0ecfe5ff5f6e"
      },
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, max_tokens, start_tokens, int2char, char2int, top_k=10, print_every=1):\n",
        "      self.max_tokens = max_tokens\n",
        "      self.start_tokens = start_tokens\n",
        "      self.int2char = int2char\n",
        "      self.char2int = char2int\n",
        "      self.print_every = print_every\n",
        "      self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "      logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "      indices = np.asarray(indices).astype(\"int32\")\n",
        "      preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "      preds = np.asarray(preds).astype(\"float32\")\n",
        "      return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "      return self.int2char[number]\n",
        "\n",
        "    def tokenize(self, symbol):\n",
        "      return self.char2int[symbol]\n",
        "  \n",
        "    def predict(self, beginning):\n",
        "      num_tokens_generated = 0\n",
        "      tokens_generated = []\n",
        "      sample_token = self.tokenize('#')\n",
        "      while num_tokens_generated <= self.max_tokens and self.detokenize(sample_token) not in end_of_sentence:\n",
        "          pad_len = max_sentence_length - len(beginning)\n",
        "          sample_index = len(beginning) - 1\n",
        "          if pad_len < 0:\n",
        "              x = beginning[:max_sentence_length]\n",
        "              sample_index = max_sentence_length - 1\n",
        "          elif pad_len > 0:\n",
        "              x = beginning + [0] * pad_len\n",
        "          else:\n",
        "              x = beginning\n",
        "          x = np.array([x])\n",
        "          y, _ = self.model.predict(x)\n",
        "          sample_token = self.sample_from(y[0][sample_index])\n",
        "          tokens_generated.append(sample_token)\n",
        "          beginning.append(sample_token)\n",
        "          num_tokens_generated = len(tokens_generated)\n",
        "      return \"\".join([self.detokenize(_) for _ in beginning])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        txt = self.predict(start_tokens)\n",
        "        print(txt)\n",
        "\n",
        "    def generate(self, beginning):\n",
        "      start_tokens = [self.tokenize(s) for s in beginning]\n",
        "      return self.predict(start_tokens)\n",
        "\n",
        "start_prompt = \"the \"\n",
        "start_tokens = [char2int.get(ch) for ch in start_prompt]\n",
        "print(start_tokens)\n",
        "text_gen_callback = TextGenerator(max_sentence_length, start_tokens, int2char, char2int)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[31, 25, 16, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZorfjYkKX79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62209f3b-8b0e-40e6-d74c-71dfd067a3c4"
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "model.fit(dataset, verbose=2, epochs=25, callbacks=[text_gen_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "175/175 - 7s - loss: 1.5310 - dense_6_loss: 1.5310\n",
            "the oratorit conellle whenorthe ond som t tr oro croowher oul staridrofe.\n",
            "Epoch 2/25\n",
            "175/175 - 4s - loss: 1.3265 - dense_6_loss: 1.3265\n",
            "the arevinghon tate that ame aid cofese coonct trorsthat oman the watite.\n",
            "Epoch 3/25\n",
            "175/175 - 4s - loss: 1.2144 - dense_6_loss: 1.2144\n",
            "the wofit ing ite olf him teer in has omougr the pitsainil butats, mit.\n",
            "Epoch 4/25\n",
            "175/175 - 4s - loss: 1.1329 - dense_6_loss: 1.1329\n",
            "the but that was o clocened turre arkad oneveasad addice, thy.\n",
            "Epoch 5/25\n",
            "175/175 - 4s - loss: 1.0751 - dense_6_loss: 1.0751\n",
            "the wont ones they tartys in marme wheres tim thing pros the cutall wouldictioove,.\n",
            "Epoch 6/25\n",
            "175/175 - 4s - loss: 1.0312 - dense_6_loss: 1.0312\n",
            "the was alway a the went, doese the the went the sepriter was by bsulle.\n",
            "Epoch 7/25\n",
            "175/175 - 4s - loss: 0.9992 - dense_6_loss: 0.9992\n",
            "the same was of so ceart think theing arted to to the prerseders that ouselopp,.\n",
            "Epoch 8/25\n",
            "175/175 - 4s - loss: 0.9737 - dense_6_loss: 0.9737\n",
            "the maghioling ole of the doowething.\n",
            "Epoch 9/25\n",
            "175/175 - 4s - loss: 0.9536 - dense_6_loss: 0.9536\n",
            "the worrds one his and asleradyeva innlying time of and about, but face the ining,.\n",
            "Epoch 10/25\n",
            "175/175 - 4s - loss: 0.9372 - dense_6_loss: 0.9372\n",
            "the weas were by to me, ovidelions, a to ter tell cart on cand at a feelsis.\n",
            "Epoch 11/25\n",
            "175/175 - 4s - loss: 0.9241 - dense_6_loss: 0.9241\n",
            "the meancach i dreect the promirse of the fursel of and the say son, scrillent.\n",
            "Epoch 12/25\n",
            "175/175 - 4s - loss: 0.9125 - dense_6_loss: 0.9125\n",
            "the barter the compter.\n",
            "Epoch 13/25\n",
            "175/175 - 4s - loss: 0.9026 - dense_6_loss: 0.9026\n",
            "the be memove had a that he tembers, stand tlughted through or then him.\n",
            "Epoch 14/25\n",
            "175/175 - 4s - loss: 0.8941 - dense_6_loss: 0.8941\n",
            "the pettled were sett perfet mysant by corenberg the oughter one countations.\n",
            "Epoch 15/25\n",
            "175/175 - 4s - loss: 0.8860 - dense_6_loss: 0.8860\n",
            "the beercame oberovna, by was sergey were it is all seeen this othen went.\n",
            "Epoch 16/25\n",
            "175/175 - 4s - loss: 0.8792 - dense_6_loss: 0.8792\n",
            "the misscorialon beather but of their with me.\n",
            "Epoch 17/25\n",
            "175/175 - 4s - loss: 0.8737 - dense_6_loss: 0.8737\n",
            "the doctors be a cond tase, letern arly were of my awith a dist withoute.\n",
            "Epoch 18/25\n",
            "175/175 - 4s - loss: 0.8677 - dense_6_loss: 0.8677\n",
            "the cannot man thores.\n",
            "Epoch 19/25\n",
            "175/175 - 4s - loss: 0.8625 - dense_6_loss: 0.8625\n",
            "the cwess of the livese to him at well on of the plams but housores and.\n",
            "Epoch 20/25\n",
            "175/175 - 4s - loss: 0.8576 - dense_6_loss: 0.8576\n",
            "the forgive folling, is the wramise interriture couctionitely of mean,.\n",
            "Epoch 21/25\n",
            "175/175 - 4s - loss: 0.8538 - dense_6_loss: 0.8538\n",
            "the self, i gan tell, how as all someter was first i had not?\n",
            "Epoch 22/25\n",
            "175/175 - 4s - loss: 0.8499 - dense_6_loss: 0.8499\n",
            "the lickes had find his stepan all and to distrance though had lowed was.\n",
            "Epoch 23/25\n",
            "175/175 - 4s - loss: 0.8464 - dense_6_loss: 0.8464\n",
            "the life, tell moried with the dedighters occase stepan and with the motherer.\n",
            "Epoch 24/25\n",
            "175/175 - 4s - loss: 0.8426 - dense_6_loss: 0.8426\n",
            "the what mistake mind she could that she said in his old of position oness,.\n",
            "Epoch 25/25\n",
            "175/175 - 4s - loss: 0.8383 - dense_6_loss: 0.8383\n",
            "the she has disive not, i diesned humiliatural ty there paitions of herself.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f52a004ef10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_IOGrPum1RZR",
        "outputId": "f812bda0-0ce5-4e02-c12d-33682cb87921"
      },
      "source": [
        "text_gen_callback.generate(\"the  \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the  twull bost had him at her each, said see, that on only and what the.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XEeQf1gr4ji8",
        "outputId": "b0bf6856-4cb7-477f-b7f4-7baf71c00e17"
      },
      "source": [
        "text_gen_callback.generate(\"good \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'good his should open it,she he recallecty was stay, in contator them, doctort.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sVJkXWuI5z8W",
        "outputId": "491424e2-65ef-49db-89a0-42b05d68d576"
      },
      "source": [
        "text_gen_callback.generate(\"transformer \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'transformer as seented over for his brrousarable with a foot hourgly been.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}